{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Overview of the program\n",
    "\n",
    "Our model is a Multi-temporal Encoder–conditioned Diffusion model trained on the Sen2-MTC dataset. It combines:\n",
    "- A CNN-based cloud encoder (processing T=3 cloudy frames)\n",
    "- A custom 4-channel UNet denoiser\n",
    "- Forward diffusion process (noise + cloudy blending)\n",
    "- Reverse sampling process that reconstructs cloud-free imagery\n",
    "\n",
    "The goal is:\n",
    "$$\n",
    "\\text{Given}: \\quad X^{(1)}_{cloudy}, X^{(2)}_{cloudy}, X^{(3)}_{cloudy} \\rightarrow \\hat{X}_{cloudless}\n",
    "$$\n",
    "\n",
    "# 2. Dataset and preprocessing\n",
    "\n",
    "## Dataset: Sen2-MTC\n",
    "- Sentinel-2 multi-temporal dataset\n",
    "- Each data sample includes:\n",
    "    - 3 cloudy frames V.S. 1 clean frame (target)\n",
    "    - Each frame has 4 channels (RGB + NIR) --> Red, Green, Blue and Near Infrared\n",
    "\t\n",
    "- Each raw data sample is in $256 \\times 256$, we crop them into $128 \\times 128$ with stride $128$\n",
    "- That is, each raw data sample are divided into $4$ non-overlapping data patch.\n",
    "- Sen2-MTC: $\\approx 13,700$ patches -> after crop $\\approx 13,700 \\times 4 = 54,800$ samples\n",
    "- Train / Val / Test split: 70%; 15%; 15%.\n",
    "\n",
    "## Multi-temporality:\n",
    "- Clouds move. \n",
    "- Single-frame data patch offer limited information about its beneath\n",
    "\t- $\\rightarrow$ Some areas are clear, some other cloudy\n",
    "\t- $\\rightarrow$ Temporal redundancy offer strong prior for cloud removal\n",
    "\t- **Solution**: Aggregate 3 frames $\\rightarrow$ Multi-temporal data."
   ],
   "id": "f77ca6ee29be4c54"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 3. Model architecture\n",
    "\n",
    "Three main components:\n",
    "\n",
    "1.\tCloud Encoder (CNN-based) \t---- extract temporal feature: cloud representation\n",
    "\n",
    "2.\tForward Diffusion (Forwarder)---- create the noisy latent for denoising\n",
    "\n",
    "3.\tDenoiser UNet \t\t\t\t---- reconstruct clean image over timesteps"
   ],
   "id": "dcf93d67f91c2308"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Model Overview](Full_Denoising_Network_Overview.png)",
   "id": "550b67dbb8417910"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Multi-temporal Cloud Encoder\n",
    "\n",
    "We process each cloudy frame individually using the same CNN encoder.\n",
    "\n",
    "For each frame $X_i$:\n",
    "$$\n",
    "F_i = \\text{FeatureMap}(X_i), \\quad z_i = \\text{Latent}(X_i)\n",
    "$$\n",
    "We compute scalar scores $s_i$ using a small MLP:\n",
    "$$\n",
    "s_i = \\text{MLP}(z_i)\n",
    "$$\n",
    "Then we feed this to yield a **temporal softmax weight**:\n",
    "$$\n",
    "w_i = \\frac{e^{s_i}}{\\sum_j e^{s_j}}\n",
    "$$\n",
    "Finally, the aggregated temporal features become:\n",
    "$$\n",
    "\\tilde{F} = \\sum_i w_i F_i, \\quad \\tilde{z} = \\sum_i w_i z_i\n",
    "$$\n",
    "Intuition:\n",
    "- This lets the model down-weight heavy-clouded frames\n",
    "- This is an idea borrowed from **Liu et al. 2025**."
   ],
   "id": "f6798e9a9f67fd3f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Cloud Encoder](Aggregation.png)",
   "id": "2b2f23d8b8ac2c93"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Forward Diffusion Process\n",
    "\n",
    "During training we have both the clean target image and the **first** cloudy frame:\n",
    "- `clean` $\\rightarrow x_0 \\in R^{B \\times C \\times H \\times W}$\n",
    "- `cloudy` $\\rightarrow x_{\\text{cloudy}} \\in R^{B \\times C \\times H \\times W}$\n",
    "- `t` $\\rightarrow$ per-sample timesteps, range $(0, \\dots, T-1)$\n",
    "\n",
    "For each sample, we find the precomputed schedules:\n",
    "\n",
    "    sigma_t     = self.sigmas[t].view(B, 1, 1, 1)\n",
    "    lambda_t    = self.lambdas[t].view(B, 1, 1, 1)\n",
    "    eps         = torch.randn_like(clean)\n",
    "\n",
    "Then we can define the **cloudy-clean interpolation mean**:\n",
    "$$\n",
    "\\mu_t = (1-\\lambda_t)x_0 + \\lambda_t x_{\\text{cloudy}}\n",
    "$$\n",
    "and the **noisy train sample**:\n",
    "$$\n",
    "x_t = \\mu_t + \\sigma_t \\epsilon, \\quad \\epsilon \\approx N(0, I)\n",
    "$$\n",
    "Thus, the `forward()` of the forwarder would return:\n",
    "- `x_t`: the corrupted image used as input to the denoiser\n",
    "- `eps`: the ground-truth noise the UNet predicts\n",
    "- `mu_t`: the clean/cloudy mixture before noise, used later in the reverse update\n",
    "\n",
    "Intuition:\n",
    "- $\\lambda_t$ controls **how much we trust** the cloudy frame v.s. the clean target at each step.\n",
    "- $\\sigma_t$ controls **how much Gaussian noise to inject**.\n",
    "- Early timesteps (smaller $t$) usually have small $\\lambda_t$ and $\\sigma_t \\rightarrow$ images close to clean.\n",
    "- At later timesteps, larger$\\lambda_t$ and $\\sigma_t \\rightarrow$ images dominated by clouds & noise.\n",
    "\n",
    "Similar to standard diffusion training, MSE loss is minimized between the true noise `eps` and the denoiser's prediction $\\hat{\\epsilon}_\\theta (x_t, t, \\tilde{F}. \\tilde{z})$, and our model **blends cloudy-clean information into the mean** $\\mu_t$."
   ],
   "id": "9de0c36c2d08c15e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Forwarder](Forwarder.png)",
   "id": "83615fe94ad815ea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Denoiser UNet (Cloud-Conditioned, 4 Channels)\n",
    "\n",
    "The **denoiser** is a 4-channel UNet that predicts the noise in the current noisy state of the image.  \n",
    "At each diffusion step we apply\n",
    "\n",
    "$$\n",
    "\\hat\\epsilon_\\theta = f_\\theta(x_t, t, z_{\\text{cloud}})\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "- $x_t \\in \\mathbb{R}^{B \\times 4 \\times H \\times W}$: current noisy image,\n",
    "- $t \\in \\{0,\\dots,T-1\\}^B$: timestep indices,\n",
    "- $z_{\\text{cloud}} \\in \\mathbb{R}^{B \\times d_{\\text{latent}}}$: cloud latent from the temporal encoder,\n",
    "- $\\hat\\epsilon_\\theta$ has the same shape as $x_t$.\n",
    "\n",
    "### Inputs and Embeddings\n",
    "\n",
    "1. **Timestep embedding**\n",
    "\n",
    "   - We normalize $t$ to $[0,1]$ and pass it through a sinusoidal + MLP stack to obtain  \n",
    "     $\\mathbf{e}_t \\in \\mathbb{R}^{B \\times d_{\\text{time}}}$.\n",
    "   - This tells the UNet *how much noise remains* at step $t$.\n",
    "\n",
    "2. **Cloud latent embedding**\n",
    "\n",
    "   - The temporal encoder produces a latent vector $z_{\\text{cloud}}$.  \n",
    "   - An MLP maps it into the same space as the time embedding, giving  \n",
    "     $\\mathbf{e}_c \\in \\mathbb{R}^{B \\times d_{\\text{time}}}$.\n",
    "   - This carries *multi-temporal information* (which areas are likely clouds vs. surface).\n",
    "\n",
    "These two embeddings are injected into every residual block of the UNet, so each block can adapt its behavior based on the current timestep and cloud context.\n",
    "\n",
    "### UNet Backbone (2-Down / 1-Mid / 2-Up)\n",
    "\n",
    "The backbone itself is a fairly standard UNet:\n",
    "\n",
    "- **Down path (encoder)**  \n",
    "  - Initial 3×3 conv maps 4 channels → `base_channels`.  \n",
    "  - Two **down blocks** (ResBlocks with conditioning) gradually increase channels and reduce spatial size (via max-pooling).\n",
    "\n",
    "- **Middle (bottleneck)**  \n",
    "  - One conditioned ResBlock at the lowest resolution, capturing global context.\n",
    "\n",
    "- **Up path (decoder)**  \n",
    "  - Two **up blocks** mirror the down path: upsampling, concatenation with skip connections, then conditioned ResBlocks.  \n",
    "  - A final 3×3 conv projects back to 4 channels, yielding \\(\\hat\\epsilon_\\theta\\).\n",
    "\n",
    "Conceptually:\n",
    "\n",
    "$$\n",
    "x_t\n",
    "\\;\\xrightarrow{\\text{encoder}}\\;\n",
    "\\text{low-res features}\n",
    "\\;\\xrightarrow{\\text{bottleneck}}\\;\n",
    "\\;\\xrightarrow{\\text{decoder + skips}}\\;\n",
    "\\hat\\epsilon_\\theta\n",
    "$$\n",
    "\n",
    "with $\\mathbf{e}_t$ and $\\mathbf{e}_c$ modulating each block.\n",
    "\n",
    "### Role in Training and Sampling\n",
    "\n",
    "- **Training:**  \n",
    "  Given $(x_t, t, z_{\\text{cloud}})$ from the forward process, the UNet predicts $\\hat\\epsilon_\\theta$.  \n",
    "  We minimize\n",
    "\n",
    "  $$\n",
    "  \\mathcal{L}_\\text{denoise} = \\left\\|\\epsilon - \\hat\\epsilon_\\theta(x_t, t, z_{\\text{cloud}})\\right\\|_2^2.\n",
    "  $$\n",
    "\n",
    "- **Sampling:**  \n",
    "  During reverse diffusion, $\\hat\\epsilon_\\theta$ is plugged into the update rule together with the schedules $\\sigma_t, \\lambda_t$ to move from $x_t$ to $x_{t-1}$, gradually removing noise and clouds until we obtain the final clean image $x_0$.\n",
    "\n",
    "In summary, the denoiser UNet is a **cloud-aware, time-aware noise predictor** that combines UNet’s spatial modeling with multi-temporal conditioning from the cloud encoder.\n"
   ],
   "id": "1877c0dd3c0087d6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![Reverse Diffusion](Diffusion_Reverse.png)",
   "id": "753835b016caa337"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 4. Evaluation Strategy\n",
    "\n",
    "To quantify how well our model removes clouds, we evaluate on the **Sen2-MTC** val/test split using four standard metrics:\n",
    "\n",
    "- **MAE** – Mean Absolute Error (lower is better)  \n",
    "- **PSNR** – Peak Signal-to-Noise Ratio (higher is better)  \n",
    "- **SSIM** – Structural Similarity Index (higher is better)  \n",
    "- **LPIPS** – Learned Perceptual Image Patch Similarity (lower is better)\n",
    "\n",
    "### How We Evaluate\n",
    "\n",
    "1. **End-to-end evaluation (`evaluate_over_loader`)**\n",
    "   - For each batch from the `test_loader`:\n",
    "     - Run `backward_sampler` to get the reconstructed clean image $x_0$.\n",
    "     - Clamp predictions and targets to $[0, \\text{max\\_val}]$.\n",
    "     - Compute MAE, PSNR, SSIM, and LPIPS (using a pretrained LPIPS network).\n",
    "   - Aggregate mean and standard deviation of each metric across the test set.\n",
    "\n",
    "2. **Precomputed evaluation (`evaluate_over_precomputed`)**\n",
    "   - For experiments where predictions are stored as `.pt` files:\n",
    "     - Load tensors `clean` and `pred`.\n",
    "     - Evaluate in batches using the same metric functions.\n",
    "   - Returns per-batch metrics and overall summary.\n",
    "\n",
    "Both paths share the same metric implementations, ensuring that on-the-fly sampling and precomputed runs are directly comparable.\n",
    "\n",
    "\n",
    "## Quantitative Results on Sen2-MTC\n",
    "\n",
    "We compare against the state-of-the-art methods reported in **Liu et al. (2025)** on the Sen2-MTC benchmark.\n",
    "- PSNR: higher better -> more accurate pixel reconstruction.\n",
    "- SSIM: Higher = better structural preservation.\n",
    "- LPIPS: Lower = more perceptual similarity to the ground truth.\n",
    "- MAE: Lower = smaller absolute pixel error.\n",
    "\n",
    "**Non-diffusion methods:**\n",
    "\n",
    "| Method              | PSNR ↑ | SSIM ↑ | LPIPS ↓ |\n",
    "|---------------------|-------:|-------:|--------:|\n",
    "| McGAN               | 17.448 | 0.513  | 0.447   |\n",
    "| Pix2Pix             | 16.985 | 0.455  | 0.535   |\n",
    "| AE                  | 15.100 | 0.441  | 0.602   |\n",
    "| STNet               | 16.206 | 0.427  | 0.503   |\n",
    "| DSen2-CR            | 16.827 | 0.534  | 0.446   |\n",
    "| STGAN               | 18.152 | 0.587  | 0.513   |\n",
    "| CTGAN               | 18.308 | 0.609  | 0.384   |\n",
    "| SEN12MS-CR-TS Net   | 18.585 | 0.615  | 0.342   |\n",
    "| PMAA                | 18.369 | 0.614  | 0.392   |\n",
    "| UnCRtainTS          | 18.770 | 0.631  | 0.333   |\n",
    "\n",
    "**Diffusion-based methods:**\n",
    "\n",
    "| Method              | PSNR ↑ | SSIM ↑ | LPIPS ↓ |\n",
    "|---------------------|-------:|-------:|--------:|\n",
    "| DDPM-CR             | 18.742 | 0.614  | 0.329   |\n",
    "| DiffCR              | 19.150 | 0.671  | 0.291   |\n",
    "| EMRDM (Liu et al.)  | 20.067 | 0.709  | 0.255   |\n",
    "| **Ours**            | **22.695** | **0.888** | **0.100** |\n",
    "\n",
    "Our model also achieves **MAE = 0.017** on the same test data.\n",
    "\n",
    "Our PSNR improvement over EMRDM +2.63 dB: a major jump compared to other models\n",
    "\n",
    "Our SSIM is 0.888, while previous highest (EMRDM) is 0.709.\n",
    "This indicates far better structural fidelity, meaning the model keeps edges, textures, and object geometry intact.\n",
    "\n",
    "Our LPIPS dropped to 0.100, while EMRDM’s is 0.255.\n",
    "Since LPIPS measures perceptual realism, the reconstructions are visually much closer to the clean target, not just numerically better.\n",
    "\n",
    "> **Conclusion:** On the Sen2-MTC benchmark, our encoder-conditioned diffusion model **outperforms all previously reported methods** across PSNR, SSIM, and LPIPS, while maintaining a very low MAE, demonstrating strong quantitative improvements in cloud removal quality.\n"
   ],
   "id": "dac86ff1ac8907b6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5. Summary\n",
    "\n",
    "Our system performs cloud removal by:\n",
    "- Taking three cloudy satellite images, \n",
    "- Encoding their shared and complementary information through a cloud encoder, and \n",
    "- Applying a guided diffusion denoising process that:\n",
    "    - Reconstructs a high-quality, cloud-free image. \n",
    "\n",
    "This multi-temporal conditioning allows the model to selectively emphasize clearer observations and suppress cloud-related artifacts across frames.\n",
    "\n",
    "Our work contributes an **encoder-conditioned diffusion model** that eliminates the need for **cloud masks** or **specialized annotations** and achieves **state-of-the-art performance** on the Sen2-MTC benchmark across PSNR, SSIM, and LPIPS. \n",
    "\n",
    "Though it can benefit more from further training on different datasets to mitigate distribution shift, our architecture is:\n",
    "- Efficient, modular, and generalizable\n",
    "- Suitable not only for cloud removal but also:\n",
    "    - Adaptable to a range of downstream remote-sensing tasks\n",
    "    - We propose one possibility right after:\n"
   ],
   "id": "213701eeda3e58c7"
  },
  {
   "cell_type": "markdown",
   "id": "42524f69",
   "metadata": {},
   "source": [
    "# Downstream Task: Evaluating Cloud Removal via DEM Height Consistency\n",
    "\n",
    "## 1. Purpose of this Task\n",
    "\n",
    "This downstream experiment evaluates whether our diffusion-based cloud-removal model preserves and restores **terrain-relevant information** in satellite RGB imagery. We measure this by feeding images into a pretrained **ImageToDEM** network (Panagiotou et al., 2020), which predicts relative Digital Elevation Models (DEMs) from single RGB inputs.\n",
    "\n",
    "Each sample contains three RGB images:\n",
    "\n",
    "- **cloudy** — original cloud-covered RGB patch  \n",
    "- **pred** — cloud-removed RGB produced by our diffusion model  \n",
    "- **clean** — cloud-free RGB image (dataset ground truth)  \n",
    "\n",
    "We generate three DEMs with the same frozen generator $G$:\n",
    "\n",
    "- $D_{\\text{cloudy}} = G(\\text{cloudy})$  \n",
    "- $D_{\\text{pred}}   = G(\\text{pred})$  \n",
    "- $D_{\\text{clean}}  = G(\\text{clean})$  (baseline reference)\n",
    "\n",
    "\n",
    "\n",
    "![Pipeline Overview](Inputs.png)\n",
    "\n",
    "### Experimental Objective\n",
    "\n",
    "If cloud removal restores spectral–spatial cues linked to terrain geometry, then\n",
    "\n",
    "$D_{\\text{pred}}$ should be much closer to $D_{\\text{clean}}$ than $D_{\\text{cloudy}}$ is.\n",
    "\n",
    "DEM similarity therefore becomes a quantitative proxy for **terrain-information recovery**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Input Preprocessing Pipeline  \n",
    "\n",
    "ImageToDEM model expects **$256\\times256$ RGB**, normalized to **$[-1,1]$**.  \n",
    "Our data is **$128\\times128\\times4$** (RGB + NIR), so we replicate the full preprocessing.\n",
    "\n",
    "\n",
    "\n",
    "### 2.1 Remove the NIR channel\n",
    "\n",
    "Keep only RGB:\n",
    "\n",
    "$\\text{rgb} = \\text{patch}[:3] \\quad (3,128,128)$\n",
    "\n",
    "\n",
    "### 2.2 Convert input to the valid $[0,255]$ range\n",
    "\n",
    "Based on value range:\n",
    "\n",
    "- If in $[0,1]$: $x = x \\cdot 255$  \n",
    "- If in $[-1,1]$: $x = (x+1)\\cdot 0.5 \\cdot 255$  \n",
    "- Already $[0,255]$: unchanged  \n",
    "\n",
    "Then clamp to valid range:\n",
    "\n",
    "$x = \\mathrm{clamp}(x,\\;0,255)$\n",
    "\n",
    "This mimics Sentinel-2 TCI preprocessing.\n",
    "\n",
    "### 2.3 Resize to $256\\times256$\n",
    "\n",
    "The U-Net inside ImageToDEM requires $256\\times256$ inputs:\n",
    "\n",
    "$x_{\\text{resized}} = \\mathrm{bilinear}(x,\\;256\\times256)$\n",
    "\n",
    "Even though original patches are $128\\times128$, we explicitly upsample them before inference.\n",
    "\n",
    "\n",
    "### 2.4 Normalize to $[-1,1]$\n",
    "\n",
    "Final normalization used by Pix2Pix and ImageToDEM:\n",
    "\n",
    "$x_{\\text{norm}} = \\dfrac{x_{\\text{resized}}}{127.5} - 1$\n",
    "\n",
    "Which gives:\n",
    "\n",
    "- $0 \\to -1$  \n",
    "- $127.5 \\to 0$  \n",
    "- $255 \\to +1$\n",
    "\n",
    "### 2.5 DEM inference\n",
    "\n",
    "Input is permuted to TensorFlow format $(H,W,C)$ and fed into $G$:\n",
    "\n",
    "$D = G(x_{\\text{norm}})$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![Pipeline Overview](Downstream_Architecture.png)\n",
    "\n",
    "\n",
    "---\n",
    "## 3. Model Architecture in the Downstream Task\n",
    "\n",
    "The DEM generator $G$ is the ImageToDEM model from Panagiotou et al. (2020):\n",
    "\n",
    "- U-Net encoder–decoder backbone with skip connections  \n",
    "- trained in a **conditional GAN (cGAN)** framework:\n",
    "  - generator: RGB $\\rightarrow$ DEM  \n",
    "  - discriminator: enforces spatial realism and height-structure coherence  \n",
    "\n",
    "Key properties:\n",
    "\n",
    "- Outputs **relative elevation fields**, not absolute heights in meters  \n",
    "- Captures terrain patterns such as slopes, ridges, and valleys  \n",
    "- Is **frozen** during our downstream experiment\n",
    "\n",
    "Because $G$ is frozen, any change in DEM quality is entirely due to the quality of the RGB images produced by our diffusion-based cloud removal.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. DEM Similarity Metric and Improvement Formula\n",
    "\n",
    "Let $D_1$ and $D_2$ be DEMs of size $H \\times W$. We use **Mean Absolute Error (MAE)**:\n",
    "\n",
    "$MAE(D_1, D_2) = \\dfrac{1}{HW} \\sum_{i,j} \\bigl| D_1(i,j) - D_2(i,j) \\bigr|$\n",
    "\n",
    "We evaluate:\n",
    "\n",
    "- $MAE_{\\text{cloudy}\\rightarrow\\text{clean}} = MAE(D_{\\text{cloudy}}, D_{\\text{clean}})$  \n",
    "- $MAE_{\\text{pred}\\rightarrow\\text{clean}}   = MAE(D_{\\text{pred}},   D_{\\text{clean}})$  \n",
    "\n",
    "The relative improvement from cloud removal is\n",
    "\n",
    "$\\text{Improvement} = 1 - \\dfrac{MAE_{\\text{pred}\\rightarrow\\text{clean}}}{MAE_{\\text{cloudy}\\rightarrow\\text{clean}}}$\n",
    "\n",
    "This quantity measures the fraction of DEM error (relative to the clean baseline) that is removed by our diffusion model.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Results\n",
    "\n",
    "**Here is a visualization example from the TEST dataset**\n",
    "\n",
    "![Pipeline Overview](Downstream_Outcome.png)\n",
    "\n",
    "**Here is a visualization example from the VAL dataset**\n",
    "\n",
    "![Pipeline Overview](Downstream_Val_Output.png)\n",
    "\n",
    "### Downstream DEM Comparison Table (Having 2050 Groups of Input for Each Dataset)\n",
    "\n",
    "| Dataset Split | MAE (Cloudy → Clean) ↓ | MAE (Pred → Clean) ↓ | Improvement ↑ |\n",
    "|--------------|-------------------------|-----------------------|----------------|\n",
    "| Validation    | 0.215648               | 0.071100              | **67.0%**      |\n",
    "| Test          | 0.217131               | 0.074932              | **65.5%**      |\n",
    "\n",
    "These values come directly from our downstream evaluation pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Interpretation\n",
    "\n",
    "### DEM behavior\n",
    "\n",
    "- $D_{\\text{cloudy}}$ often collapses into a flat or noisy height field because clouds obscure the spectral patterns required for elevation estimation.  \n",
    "- $D_{\\text{pred}}$ recovers gradient structure and ridge–valley morphology that closely resembles $D_{\\text{clean}}$.  \n",
    "- This indicates that our diffusion-based cloud removal reintroduces terrain-consistent cues that the DEM model can exploit.\n",
    "\n",
    "### Scientific meaning\n",
    "\n",
    "Because $G$ is frozen:\n",
    "\n",
    "- Any reduction in $MAE_{\\text{pred}\\rightarrow\\text{clean}}$ compared with $MAE_{\\text{cloudy}\\rightarrow\\text{clean}}$ must come from improved RGB inputs.  \n",
    "- A **65–67% reduction** in DEM discrepancy means that our model recovers most of the terrain information that was lost due to cloud cover.\n",
    "\n",
    "### Final Conclusion\n",
    "\n",
    "Our diffusion-based cloud-removal model restores approximately **65–67%** of terrain information lost under clouds, demonstrating that visually cleaned images produced by our method are significantly more useful for downstream DEM estimation and geospatial analysis than the original cloudy imagery.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
