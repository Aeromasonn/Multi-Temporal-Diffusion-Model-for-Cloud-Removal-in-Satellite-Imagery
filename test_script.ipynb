{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:18:57.231273Z",
     "start_time": "2025-11-23T22:18:48.985831Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import tifffile as tiff\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchvision.transforms.v2.functional import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4e2c3dcc2f4d920",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:18:57.262418Z",
     "start_time": "2025-11-23T22:18:57.237845Z"
    }
   },
   "outputs": [],
   "source": [
    "class SatImage_Dataloader(Dataset):\n",
    "    \"\"\"\n",
    "    Mono-temporal Sen2-MTC dataset with patch extraction.\n",
    "\n",
    "    For each time index n:\n",
    "        cloudy[n] has multiple *.tif files\n",
    "        cloudless[n] has one  *.tif file\n",
    "\n",
    "    One cloudy sample for each n -> mono-temporal.\n",
    "    If patch_size is provided:\n",
    "        If center_crop True : return crop from the center of image.\n",
    "        If center_crop False: return a random crop on the image.\n",
    "\n",
    "    If stride is provided:\n",
    "        Enumerate all patches of size patch_size with given stride.\n",
    "        (This multiplies dataset size with deterministic patches.)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, route, patch_size=128, stride=128, center_crop=False, transform=None):\n",
    "        super().__init__()\n",
    "        self.root_dir = route\n",
    "        self.patch_size = patch_size\n",
    "        self.center_crop = center_crop\n",
    "        self.transform = transform\n",
    "        self.stride = stride  # NEW\n",
    "\n",
    "        # List of samples:\n",
    "        # If stride is None:       (cloudy_path, clean_path)\n",
    "        # If stride is not None:   (cloudy_path, clean_path, top, left)\n",
    "        self.samples = []\n",
    "\n",
    "        cloudy_pattern    = r\"(.+?)_(\\d+)_(\\d+)\\.tif\"       # matches n_k (cloud)\n",
    "        cloudless_pattern = r\"(.+?)_(\\d+)\\.tif\"            # matches n (clean)\n",
    "\n",
    "        for tile_name in sorted(os.listdir(route)):\n",
    "            tile_path = os.path.join(route, tile_name)\n",
    "            cloud_dir = os.path.join(tile_path, \"cloud\")\n",
    "            clean_dir = os.path.join(tile_path, \"cloudless\")\n",
    "\n",
    "            if not (os.path.isdir(cloud_dir) and os.path.isdir(clean_dir)):\n",
    "                continue\n",
    "\n",
    "            # 1 — Parse cloudy files grouped by time index n\n",
    "            cloudy_by_n = {}\n",
    "            for fname in sorted(os.listdir(cloud_dir)):\n",
    "                if not fname.endswith(\".tif\"):\n",
    "                    continue\n",
    "                m = re.match(cloudy_pattern, fname)\n",
    "                if not m:\n",
    "                    continue\n",
    "\n",
    "                n = int(m.group(2))\n",
    "                path = os.path.join(cloud_dir, fname)\n",
    "                cloudy_by_n.setdefault(n, []).append(path)\n",
    "\n",
    "            # 2 — Parse cloudless (clean) files by time index n\n",
    "            clean_by_n = {}\n",
    "            for fname in sorted(os.listdir(clean_dir)):\n",
    "                if not fname.endswith(\".tif\"):\n",
    "                    continue\n",
    "                m = re.match(cloudless_pattern, fname)\n",
    "                if not m:\n",
    "                    continue\n",
    "\n",
    "                n = int(m.group(2))\n",
    "                clean_by_n[n] = os.path.join(clean_dir, fname)\n",
    "\n",
    "            # 3 — For each n, pick *one* cloudy and match with clean[n]\n",
    "            for n in sorted(cloudy_by_n.keys()):\n",
    "                if n not in clean_by_n:\n",
    "                    print(f\"[WARNING] Tile {tile_name}: time {n} has cloudy but no clean.\")\n",
    "                    continue\n",
    "\n",
    "                cloudy_path = cloudy_by_n[n][0]   # mono-temporal choose first\n",
    "                clean_path = clean_by_n[n]\n",
    "\n",
    "                # If stride is not provided → old behavior: 1 sample per image\n",
    "                if self.stride is None:\n",
    "                    self.samples.append((cloudy_path, clean_path, None, None))\n",
    "                    continue\n",
    "\n",
    "                # Otherwise enumerate patches using stride\n",
    "                # We must load image shape\n",
    "                tmp = tiff.imread(cloudy_path)  # (H, W, C)\n",
    "                H, W, _ = tmp.shape\n",
    "\n",
    "                ps = self.patch_size\n",
    "                st = self.stride\n",
    "\n",
    "                for top in range(0, H - ps + 1, st):\n",
    "                    for left in range(0, W - ps + 1, st):\n",
    "                        self.samples.append((cloudy_path, clean_path, top, left))\n",
    "\n",
    "        print(f\"[Sen2MTC loaded] Total samples (including patches): {len(self.samples)}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def load_tif(self, path):\n",
    "        arr = tiff.imread(path)  # (H, W, C)\n",
    "        arr = np.array(arr, dtype=np.float32)\n",
    "        return arr\n",
    "\n",
    "    # Patch extraction helper\n",
    "    def extract_patch(self, img, size):\n",
    "        \"\"\"\n",
    "        img: numpy array shape (C,H,W)\n",
    "        size: int, patch size\n",
    "        returns: (C, size, size)\n",
    "        \"\"\"\n",
    "        _, H, W = img.shape\n",
    "        if size > H or size > W:\n",
    "            raise ValueError(f\"Patch size {size} > image size {(H,W)}\")\n",
    "\n",
    "        if self.center_crop:\n",
    "            top = (H - size) // 2\n",
    "            left = (W - size) // 2\n",
    "        else:\n",
    "            top = np.random.randint(0, H - size + 1)\n",
    "            left = np.random.randint(0, W - size + 1)\n",
    "\n",
    "        patch = img[:, top:top+size, left:left+size]\n",
    "        return patch\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        cloudy_path, clean_path, top, left = self.samples[idx]\n",
    "\n",
    "        cloudy = self.load_tif(cloudy_path)\n",
    "        clean  = self.load_tif(clean_path)\n",
    "\n",
    "        cloudy = torch.from_numpy(cloudy.transpose(2,0,1))\n",
    "        clean  = torch.from_numpy(clean.transpose(2,0,1))\n",
    "\n",
    "        # Patch extraction\n",
    "        if self.patch_size is not None:\n",
    "            if self.stride is not None:\n",
    "                # predetermined patch from (top, left)\n",
    "                ps = self.patch_size\n",
    "                cloudy = cloudy[:, top:top+ps, left:left+ps]\n",
    "                clean  = clean[:, top:top+ps, left:left+ps]\n",
    "            else:\n",
    "                # center or random crop\n",
    "                cloudy = self.extract_patch(cloudy, self.patch_size)\n",
    "                clean  = self.extract_patch(clean, self.patch_size)\n",
    "\n",
    "        sample = {\n",
    "            \"cloudy\": cloudy,\n",
    "            \"clean\": clean,\n",
    "            \"cloudy_path\": cloudy_path,\n",
    "            \"clean_path\": clean_path\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9db1e26fbe116d7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:18:59.527897Z",
     "start_time": "2025-11-23T22:18:57.706368Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "route = 'D:\\Desktop\\Duke 2025 Fall\\CS 372\\Final Project\\372 Data\\CTGAN\\Sen2_MTC\\dataset\\Sen2_MTC'    # route to data\n",
    "size = 128              # if size = patch_size = stride -> no overlapping sampling\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c003bd5cc495c50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:18:59.581699Z",
     "start_time": "2025-11-23T22:18:59.575781Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_global_stats(dataset, batch_size=32):\n",
    "    \"\"\"\n",
    "    Compute global per-channel mean and std over a dataset.\n",
    "    Returns:\n",
    "        mean, std\n",
    "    \"\"\"\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    channel_sum = None\n",
    "    channel_sq_sum = None\n",
    "    total_pixels = 0\n",
    "\n",
    "    for batch in loader:\n",
    "        x = batch[\"cloudy\"].double()   # (B,C,H,W)\n",
    "        B, C, H, W = x.shape\n",
    "\n",
    "        if channel_sum is None:\n",
    "            channel_sum = torch.zeros(C, dtype=torch.float64)\n",
    "            channel_sq_sum = torch.zeros(C, dtype=torch.float64)\n",
    "\n",
    "        # Sum over batch and spatial dims\n",
    "        channel_sum += x.sum(dim=[0, 2, 3])\n",
    "        channel_sq_sum += (x ** 2).sum(dim=[0, 2, 3])\n",
    "\n",
    "        total_pixels += B * H * W\n",
    "\n",
    "    mean = channel_sum / total_pixels\n",
    "    std = torch.sqrt(channel_sq_sum / total_pixels - mean**2)\n",
    "\n",
    "    print(\"Global mean:\", mean)\n",
    "    print(\"Global std:\", std)\n",
    "\n",
    "    return mean.float(), std.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5a5c575620230fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:18:59.599751Z",
     "start_time": "2025-11-23T22:18:59.590626Z"
    }
   },
   "outputs": [],
   "source": [
    "class Normalization:\n",
    "    \"\"\"\n",
    "    Normalize cloudy and clean patches using precomputed mean/std.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean.reshape(-1, 1, 1)   # (C,1,1)\n",
    "        self.std  = std.reshape(-1, 1, 1)    # (C,1,1)\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        cloudy = sample[\"cloudy\"]\n",
    "        clean  = sample[\"clean\"]\n",
    "\n",
    "        cloudy_n = (cloudy - self.mean) / (self.std + 1e-6)\n",
    "        clean_n  = (clean  - self.mean) / (self.std + 1e-6)\n",
    "\n",
    "        return {\n",
    "            \"cloudy\": cloudy_n,\n",
    "            \"clean\": clean_n,\n",
    "            \"cloudy_path\": sample[\"cloudy_path\"],\n",
    "            \"clean_path\": sample[\"clean_path\"],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a5bb777a2496a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:19:43.243216Z",
     "start_time": "2025-11-23T22:18:59.638525Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: 'D:\\\\Desktop\\\\Duke 2025 Fall\\\\CS 372\\\\Final Projectú Data\\\\CTGAN\\\\Sen2_MTC\\\\dataset\\\\Sen2_MTC'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dataset_raw \u001b[38;5;241m=\u001b[39m SatImage_Dataloader(route\u001b[38;5;241m=\u001b[39mroute, patch_size\u001b[38;5;241m=\u001b[39msize, stride\u001b[38;5;241m=\u001b[39msize)\n\u001b[0;32m      2\u001b[0m mean, std \u001b[38;5;241m=\u001b[39m compute_global_stats(dataset_raw, batch_size\u001b[38;5;241m=\u001b[39mbatch_size)\n\u001b[0;32m      3\u001b[0m normalized \u001b[38;5;241m=\u001b[39m Normalization(mean, std)\n",
      "Cell \u001b[1;32mIn[3], line 35\u001b[0m, in \u001b[0;36mSatImage_Dataloader.__init__\u001b[1;34m(self, route, patch_size, stride, center_crop, transform)\u001b[0m\n\u001b[0;32m     32\u001b[0m cloudy_pattern    \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)_(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)_(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m       \u001b[38;5;66;03m# matches n_k (cloud)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m cloudless_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(.+?)_(\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m.tif\u001b[39m\u001b[38;5;124m\"\u001b[39m            \u001b[38;5;66;03m# matches n (clean)\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tile_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(os\u001b[38;5;241m.\u001b[39mlistdir(route)):\n\u001b[0;32m     36\u001b[0m     tile_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(route, tile_name)\n\u001b[0;32m     37\u001b[0m     cloud_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(tile_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcloud\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: 'D:\\\\Desktop\\\\Duke 2025 Fall\\\\CS 372\\\\Final Projectú Data\\\\CTGAN\\\\Sen2_MTC\\\\dataset\\\\Sen2_MTC'"
     ]
    }
   ],
   "source": [
    "dataset_raw = SatImage_Dataloader(route=route, patch_size=size, stride=size)\n",
    "mean, std = compute_global_stats(dataset_raw, batch_size=batch_size)\n",
    "normalized = Normalization(mean, std)\n",
    "\n",
    "dataset = SatImage_Dataloader(route=route, patch_size=size, stride=size, transform=normalized)\n",
    "train_ratio=.7; val_ratio=.15; test_ratio=.15\n",
    "\n",
    "total       = len(dataset)\n",
    "train_len   = int(train_ratio*total)\n",
    "val_len     = int(val_ratio*total)+1\n",
    "test_len    = int(test_ratio*total)\n",
    "\n",
    "generator = torch.Generator().manual_seed(2025)\n",
    "train_set, val_set, test_set = random_split(dataset, \n",
    "                                            [train_len, val_len, test_len],\n",
    "                                            generator=generator) #reproducability\n",
    "\n",
    "train_loader    = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "val_loader      = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "test_loader     = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "#expect ~40s run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3c55c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'D:/Desktop/Duke 2025 Fall/CS 372/Final Project Data/CTGAN/Sen2_MTC/dataset/Sen2_MTC'\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "route = \"D:/Desktop/Duke 2025 Fall/CS 372/Final Project Data/CTGAN/Sen2_MTC/dataset/Sen2_MTC\"\n",
    "\n",
    "print(repr(route))\n",
    "print(os.path.exists(route))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8f170bbc63a342",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:19:43.323434Z",
     "start_time": "2025-11-23T22:19:43.253983Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 128, 128])\n",
      "torch.Size([16, 4, 128, 128])\n",
      "tensor(-0.0600) tensor(0.9697)\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    print(batch['cloudy'].shape)\n",
    "    print(batch['clean'].shape)\n",
    "    x = batch['cloudy']\n",
    "    print(x.mean(), x.std())\n",
    "    break\n",
    "#[batch_size, channels, height, width] -> [batch_size, channels, patch_size, patch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d656281ba1a4050",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:19:43.403908Z",
     "start_time": "2025-11-23T22:19:43.388559Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# structure ok\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Small residual conv block:\n",
    "    in -> Conv -> GN -> SiLU -> Conv -> GN -> +skip\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, num_groups=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.gn1   = nn.GroupNorm(num_groups=min(num_groups, out_channels), num_channels=out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.gn2   = nn.GroupNorm(num_groups=min(num_groups, out_channels), num_channels=out_channels)\n",
    "        self.silu   = nn.SiLU(inplace=True)\n",
    "\n",
    "        # if channel dims change, use 1*1 conv for skip\n",
    "        if in_channels != out_channels:\n",
    "            self.skip = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.skip = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.skip(x)\n",
    "        out = self.conv1(x)\n",
    "        out = self.gn1(out)\n",
    "        out = self.silu(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.gn2(out)\n",
    "        out = out + identity\n",
    "        out = self.silu(out)\n",
    "        return out\n",
    "\n",
    "class CloudEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Cloud encoder based on CNN.\n",
    "\n",
    "    Input:\n",
    "        x: (B, in_channels, H, W), normalize required\n",
    "\n",
    "    Output:\n",
    "        z: (B, latent_dim) cloud embeddings\n",
    "\n",
    "    Structure:\n",
    "        - conv\n",
    "        - #num_stages residual + downsample stages\n",
    "        - Final residual block\n",
    "        - Global average pooling\n",
    "        - Linear projection to latent_dim\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels = 4,\n",
    "        base_channels = 32,\n",
    "        num_stages = 3,\n",
    "        latent_dim = 128,\n",
    "        num_groups = 8,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.base_channels = base_channels\n",
    "        self.num_stages = num_stages\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        layers = []\n",
    "\n",
    "        # Initial conv to get to base_channels\n",
    "        layers.append(\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(in_channels, base_channels, kernel_size=3, padding=1),\n",
    "                nn.GroupNorm(num_groups=min(num_groups, base_channels), num_channels=base_channels),\n",
    "                nn.SiLU(inplace=True),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        in_ch = base_channels\n",
    "        channels = [base_channels * (2 ** i) for i in range(num_stages)]\n",
    "\n",
    "        # Residual + downsample stages\n",
    "        self.down_blocks = nn.ModuleList()\n",
    "        self.downsamples = nn.ModuleList()\n",
    "\n",
    "        for out_ch in channels:\n",
    "            self.down_blocks.append(ResidualBlock(in_ch, out_ch, num_groups=num_groups))\n",
    "            # stride-2 conv for downsampling\n",
    "            self.downsamples.append(\n",
    "                nn.Conv2d(out_ch, out_ch, kernel_size=4, stride=2, padding=1)\n",
    "            )\n",
    "            in_ch = out_ch\n",
    "\n",
    "        # A final residual block at lowest resolution\n",
    "        self.final_block = ResidualBlock(in_ch, in_ch, num_groups=num_groups)\n",
    "\n",
    "        # Register first stem as single module for clarity\n",
    "        self.stem = layers[0]\n",
    "        \n",
    "        # Projection to latent space\n",
    "        self.proj = nn.Linear(in_ch, latent_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x: (B, C, H, W)\n",
    "        returns:\n",
    "            z: (B, latent_dim)\n",
    "        \"\"\"\n",
    "        # Stem\n",
    "        h = self.stem(x)  # (B, base_channels, H, W)\n",
    "\n",
    "        # Downsampling stages\n",
    "        for block, down in zip(self.down_blocks, self.downsamples):\n",
    "            h = block(h)\n",
    "            h = down(h)  # spatial size halves each time\n",
    "\n",
    "        # Final block\n",
    "        h = self.final_block(h)\n",
    "\n",
    "        # Global average pooling over spatial dims: (B, C, 1, 1) -> (B, C)\n",
    "        h = F.adaptive_avg_pool2d(h, output_size=1).squeeze(-1).squeeze(-1)\n",
    "\n",
    "        # Project to latent_dim\n",
    "        z = self.proj(h)  # (B, latent_dim)\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afc3118fadb98551",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:19:43.454756Z",
     "start_time": "2025-11-23T22:19:43.447613Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_noise_schedule(T=500, sigma_min=1e-4, sigma_max=.02):\n",
    "    sigmas = torch.linspace(sigma_min, sigma_max, T)\n",
    "    return sigmas\n",
    "\n",
    "def sample_timesteps(batch_size, T, device):\n",
    "    out = torch.randint(low=1, high=T, size=(batch_size,)).to(device)   \n",
    "    return out\n",
    "\n",
    "def forward_diffusion(clean, cloudy, t, sigmas):\n",
    "    \"\"\"\n",
    "    Mean-Reverting Diffusion (MRDM) forward process:\n",
    "        x_t = cloudy + sigma_t * eps\n",
    "        cloudy as mean, drive noise schedule to approximate cloudy mean\n",
    "\n",
    "    Args:\n",
    "        clean:   (B, C, H, W) clean images (only needed for generating eps)\n",
    "        cloudy:  (B, C, H, W) cloudy images (used as drift center μ_t)\n",
    "        t:       (B,) integer timesteps\n",
    "        sigmas:  (T,) schedule of σ_t\n",
    "\n",
    "    Returns:\n",
    "        x_t:   noisy image\n",
    "        eps:   noise used in generation (target for diffusion loss)\n",
    "        mu_t:  drift mean (cloudy)\n",
    "    \"\"\"\n",
    "\n",
    "    B = clean.shape[0]\n",
    "    # Sample Gaussian noise\n",
    "    eps = torch.randn_like(clean)\n",
    "\n",
    "    sigma_t = sigmas[t].view(B, 1, 1, 1)\n",
    "    mu_t = cloudy\n",
    "    x_t = mu_t + sigma_t * eps\n",
    "\n",
    "    return x_t, eps, mu_t\n",
    "\n",
    "class ForwardDiffusion(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper.\n",
    "\n",
    "    Usage:\n",
    "        forwarder = ForwardDiffusionMRDM(T=1000)\n",
    "        x_t, eps, mu_t = forwarder(clean, cloudy, t)\n",
    "    \"\"\"\n",
    "    def __init__(self, T=1000, sigma_min=0.0001, sigma_max=0.02):\n",
    "        super().__init__()\n",
    "        self.T = T\n",
    "\n",
    "        # register sigmas as buffer so they move with .to(device)\n",
    "        sigmas = make_noise_schedule(T=T, sigma_min=sigma_min, sigma_max=sigma_max)\n",
    "        self.register_buffer(\"sigmas\", sigmas)\n",
    "\n",
    "    def sample_t(self, batch_size, device):\n",
    "        return sample_timesteps(batch_size, self.T, device)\n",
    "\n",
    "    def forward(self, clean, cloudy, t):\n",
    "        return forward_diffusion(clean, cloudy, t, self.sigmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f645ce0d70f1f49b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:20:12.314913Z",
     "start_time": "2025-11-23T22:19:43.491690Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\PyCharm 2024.2.1\\Projects\\pythonProject\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from diffusers import UNet2DModel\n",
    "\n",
    "class CloudConditionedUNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Wrapper around Hugging Face UNet2DModel with:\n",
    "      - in_channels = 4, out_channels = 4\n",
    "      - conditioning on cloud embedding z_cloud via a per-channel bias\n",
    "      - internal timestep embedding (no t_emb needed externally)\n",
    "\n",
    "    Forward signature:\n",
    "        eps_pred = model(x_t, t, z_cloud)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        base_model_name: str = \"google/ddpm-cifar10-32\",\n",
    "        in_channels: int = 4,\n",
    "        out_channels: int = 4,\n",
    "        latent_dim: int = 128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Load pretrained UNet (3->3 by default)\n",
    "        base_unet = UNet2DModel.from_pretrained(base_model_name)\n",
    "\n",
    "        # 2. Copy config and modify channels\n",
    "        config = base_unet.config\n",
    "        config.in_channels = in_channels\n",
    "        config.out_channels = out_channels\n",
    "\n",
    "        # 3. Rebuild UNet with same architecture, but new channel counts\n",
    "        self.unet = UNet2DModel.from_config(config)\n",
    "\n",
    "        # 4. Optionally, copy overlapping weights for conv_in/conv_out (3 -> 4 channels)\n",
    "        with torch.no_grad():\n",
    "            # conv_in: (C_out, C_in, k, k)\n",
    "            old_w = base_unet.conv_in.weight    # (C_out, 3, k, k)\n",
    "            new_w = self.unet.conv_in.weight    # (C_out, 4, k, k)\n",
    "            new_w[:, :3, :, :] = old_w\n",
    "            # last channel (index 3) stays random-initialized\n",
    "\n",
    "            self.unet.conv_in.bias.data.copy_(base_unet.conv_in.bias.data)\n",
    "\n",
    "            # conv_out: (4, C_in, k, k) vs old (3, C_in, k, k)\n",
    "            old_w_out = base_unet.conv_out.weight   # (3, C_in, k, k)\n",
    "            new_w_out = self.unet.conv_out.weight   # (4, C_in, k, k)\n",
    "            new_w_out[:3, :, :, :] = old_w_out\n",
    "            # 4th channel random-init\n",
    "            self.unet.conv_out.bias.data[:3] = base_unet.conv_out.bias.data\n",
    "\n",
    "        # 5. Conditioning MLP: z_cloud -> per-channel bias on input\n",
    "        self.cond_proj = nn.Linear(latent_dim, in_channels)\n",
    "\n",
    "    def forward(self, x_t: torch.Tensor, t: torch.Tensor, z_cloud: torch.Tensor):\n",
    "        \"\"\"\n",
    "        x_t:      (B, 4, H, W)  noisy sample at timestep t\n",
    "        t:        (B,) or scalar int/float timestep; UNet2DModel embeds this internally\n",
    "        z_cloud:  (B, latent_dim) from your CloudEncoder\n",
    "\n",
    "        Returns:\n",
    "            eps_pred: (B, 4, H, W) predicted noise\n",
    "        \"\"\"\n",
    "\n",
    "        # project cloud embedding to per-channel bias\n",
    "        cond = self.cond_proj(z_cloud)         # (B, 4)\n",
    "        cond = cond.unsqueeze(-1).unsqueeze(-1)  # (B, 4, 1, 1)\n",
    "\n",
    "        # inject conditioning as bias on input channels\n",
    "        x_cond = x_t + cond\n",
    "\n",
    "        # HuggingFace UNet2DModel handles timestep embedding internally\n",
    "        out = self.unet(sample=x_cond, timestep=t)\n",
    "\n",
    "        # out is a UNet2DOutput; main tensor is .sample\n",
    "        eps_pred = out.sample\n",
    "        return eps_pred\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9715b901ec210980",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:27:21.682315Z",
     "start_time": "2025-11-23T22:27:21.665052Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 1. Simple sinusoidal timestep embedding\n",
    "# ---------------------------------------------------------------------\n",
    "def timestep_embedding(t, dim):\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        torch.arange(half, dtype=torch.float32, device=t.device)\n",
    "        * (-torch.log(torch.tensor(10000.0)) / (half - 1))\n",
    "    )\n",
    "    args = t[:, None].float() * freqs[None]\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=-1)\n",
    "    return emb\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 2. Basic ResNet block (Conv-Norm-Act-Conv-Norm-Act)\n",
    "# ---------------------------------------------------------------------\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim):\n",
    "        super().__init__()\n",
    "        self.time_dense = nn.Linear(time_emb_dim, out_ch)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.norm1 = nn.GroupNorm(8, out_ch)\n",
    "        self.norm2 = nn.GroupNorm(8, out_ch)\n",
    "        self.act = nn.SiLU()\n",
    "\n",
    "        # shortcut if needed\n",
    "        self.shortcut = (\n",
    "            nn.Conv2d(in_ch, out_ch, 1)\n",
    "            if in_ch != out_ch\n",
    "            else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t_emb):\n",
    "        \"\"\"\n",
    "        x:     (B, C, H, W)\n",
    "        t_emb: (B, time_emb_dim)\n",
    "        \"\"\"\n",
    "        h = self.conv1(x)\n",
    "        h = self.norm1(h)\n",
    "        h = self.act(h)\n",
    "\n",
    "        # add time embedding\n",
    "        t_added = self.time_dense(t_emb)[:, :, None, None]\n",
    "        h = h + t_added\n",
    "\n",
    "        h = self.conv2(h)\n",
    "        h = self.norm2(h)\n",
    "        h = self.act(h)\n",
    "\n",
    "        return h + self.shortcut(x)\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------\n",
    "# 3. Minimal UNet for 4 channel diffusion\n",
    "# ---------------------------------------------------------------------\n",
    "class SmallUNet4C(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels=4,\n",
    "        out_channels=4,\n",
    "        base_channels=64,\n",
    "        time_emb_dim=256,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # time embedding MLP\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "        )\n",
    "\n",
    "        # ---------- Downsampling ----------\n",
    "        self.conv_in = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n",
    "\n",
    "        self.down1 = ResBlock(base_channels, base_channels * 2, time_emb_dim)\n",
    "        self.down2 = ResBlock(base_channels * 2, base_channels * 4, time_emb_dim)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "\n",
    "        # ---------- Middle ----------\n",
    "        self.mid = ResBlock(base_channels * 4, base_channels * 4, time_emb_dim)\n",
    "\n",
    "        # ---------- Upsampling ----------\n",
    "        self.up1 = ResBlock(base_channels * 4 + base_channels * 4, base_channels * 2, time_emb_dim)\n",
    "        self.up2 = ResBlock(base_channels * 2 + base_channels * 2, base_channels, time_emb_dim)\n",
    "\n",
    "        self.conv_out = nn.Conv2d(base_channels, out_channels, 3, padding=1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # t: integer or float timestep\n",
    "        if len(t.shape) == 0:\n",
    "            t = t.unsqueeze(0)\n",
    "        t_emb = timestep_embedding(t, self.time_mlp[0].in_features)\n",
    "        t_emb = self.time_mlp(t_emb)\n",
    "\n",
    "        # Down\n",
    "        x1 = self.conv_in(x)            # (B, 64, H, W)\n",
    "        x2 = self.down1(x1, t_emb)      # (B, 128, H, W)\n",
    "        x3 = self.pool(x2)\n",
    "        x3 = self.down2(x3, t_emb)      # (B, 256, H/2, W/2)\n",
    "\n",
    "        # Middle\n",
    "        xm = self.mid(x3, t_emb)\n",
    "\n",
    "        # Up\n",
    "        u1 = torch.cat([xm, x3], dim=1)\n",
    "        u1 = self.up1(u1, t_emb)        # (B, 128, H/2, W/2)\n",
    "\n",
    "        u2 = F.interpolate(u1, scale_factor=2, mode=\"nearest\")\n",
    "        u2 = torch.cat([u2, x2], dim=1)\n",
    "        u2 = self.up2(u2, t_emb)        # (B, 64, H, W)\n",
    "\n",
    "        out = self.conv_out(u2)\n",
    "        return out\n",
    "\n",
    "class CloudConditionedUNet_4C(nn.Module):\n",
    "    def __init__(self, in_channels=4, out_channels=4, latent_dim=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.unet = SmallUNet4C(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "        )\n",
    "\n",
    "        self.cond_proj = nn.Linear(latent_dim, in_channels)\n",
    "\n",
    "    def forward(self, x_t, t, z_cloud):\n",
    "        cond = self.cond_proj(z_cloud).unsqueeze(-1).unsqueeze(-1)\n",
    "        x_cond = x_t + cond\n",
    "        eps_pred = self.unet(x_cond, t)\n",
    "        return eps_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dba6c0f637faa891",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-23T22:54:15.968420Z",
     "start_time": "2025-11-23T22:50:58.844149Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n",
      "batch complete\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] loss = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m temp_train \u001b[38;5;241m=\u001b[39m DataLoader(test_set, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m \u001b[43mforward_trainer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemp_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloud_encoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdenoiser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 49\u001b[0m, in \u001b[0;36mforward_trainer\u001b[1;34m(epochs, train_loader, optimizer, cloud_encoder, denoiser, device)\u001b[0m\n\u001b[0;32m     46\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 49\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m num_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch complete\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "forwarder = ForwardDiffusion(T=500).to(device)\n",
    "cloud_encoder = CloudEncoder(\n",
    "    in_channels=4,\n",
    "    base_channels=32,\n",
    "    num_stages=2,\n",
    "    latent_dim=128\n",
    ").to(device)\n",
    "\"\"\"\n",
    "denoiser = CloudConditionedUNet(\n",
    "    base_model_name=\"google/ddpm-cifar10-32\",\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    latent_dim=128\n",
    ").to(device)\n",
    "\"\"\"\n",
    "denoiser = CloudConditionedUNet_4C(\n",
    "    in_channels=4,\n",
    "    out_channels=4,\n",
    "    latent_dim=128\n",
    ").to(device)\n",
    "\n",
    "\n",
    "params = list(cloud_encoder.parameters()) + list(denoiser.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)\n",
    "epochs = 1\n",
    "\n",
    "def forward_trainer(epochs, train_loader, optimizer, cloud_encoder, denoiser, device):\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            cloudy = batch['cloudy'].to(device)\n",
    "            clean  = batch['clean'].to(device)\n",
    "            B = cloudy.shape[0]\n",
    "\n",
    "            t = forwarder.sample_t(B, device=device)\n",
    "            x_t, eps, mu_t = forwarder(clean, cloudy, t)\n",
    "            z_cloud = cloud_encoder(cloudy)\n",
    "            eps_pred = denoiser(x_t, t, z_cloud)\n",
    "        \n",
    "            loss = torch.mean((eps_pred - eps)**2)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        print(f\"[Epoch {epoch}] loss = {avg_loss:.6f}\")\n",
    "\n",
    "temp_train = DataLoader(test_set, batch_size=32, shuffle=True, num_workers=0)\n",
    "forward_trainer(epochs, temp_train, optimizer, cloud_encoder, denoiser, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1bd062237a2ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
